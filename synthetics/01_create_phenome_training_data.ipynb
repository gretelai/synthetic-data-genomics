{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "base_path = pathlib.Path(os.getcwd().replace(\"/synthetics\", \"\"))\n",
    "experiment_path = base_path / 'mice_data_set' / 'out'\n",
    "    \n",
    "phenomes = pd.concat(map(pd.read_csv, experiment_path.glob('pheno_*.csv')))\n",
    "phenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # Drop NaN fields\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Filter out invalid measurements\n",
    "    df = df[((df[\"discard\"] == \"no\") & (df[\"mixup\"] == \"no\"))]\n",
    "    \n",
    "    # Remove non-relevant fields\n",
    "    irrelevant_fields = pd.read_csv('irrelevant-fields.csv')\n",
    "    relevant_fields = list(set(phenomes.columns) - set(irrelevant_fields['FIELDS']))\n",
    "    df = df.filter(relevant_fields)\n",
    "    \n",
    "    # Remove unnecessary precision\n",
    "    df = df.round(2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "phenomes = clean_dataframe(phenomes)\n",
    "phenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write synthetic training set to CSV\n",
    "phenomes.to_csv(base_path / 'mice_data_set' / 'data' / 'phenomes_train.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually batch training from matching covariates and write to CSV\n",
    "def create_manual_batches(df: pd.DataFrame):\n",
    "\n",
    "    batches = [\n",
    "        ['BMD', 'SW16', 'abBMD', 'TA', 'tibia', 'EDL', 'soleus', 'plantaris', 'gastroc', 'SW6', 'sacweight', 'SW17', 'testisweight', 'methage', 'PPIweight', 'bw1'],\n",
    "        ['bw2', 'bw3', 'bw0', 'taillength', 'SW3', 'D3ctrtime0to30', 'D3ctrtime0to15', 'SW11', 'fastglucose', 'SW1', 'glucoseage', 'SW19', 'SW7', 'SW22', 'SW14', 'SW24', 'SW20', 'SW10', 'SW9', 'SW4'],\n",
    "        ['AvToneD3', 'AvContextD2', 'AvToneD1', 'PreTrainD1', 'PPIbox1', 'FCbox1', 'methcage9', 'PPIbox2', 'PPIbox3', 'FCbox2', 'methcage10', 'methcage11', 'FCbox3', 'PPIbox4', 'methcage7', 'methcage8', 'methcage12'],\n",
    "        ['D3vact0to30', 'D3vact0to15', 'D1vact0to15', 'D1vact0to30', 'D2vact0to15', 'D2vact0to30', 'D2ctrtime0to15', 'D2ctrtime0to30', 'D1ctrtime0to30', 'D1ctrtime0to15'],\n",
    "        ['D1hact0to15', 'D1hact0to30', 'D2hact0to30', 'D2hact0to15', 'D2TOTDIST5', 'D2TOTDIST15', 'D2TOTDIST10', 'D2totaldist0to15', 'D2totaldist0to30', 'D2TOTDIST20', 'D2TOTDIST30', 'D2TOTDIST25', 'D1TOTDIST25', 'D1TOTDIST30', 'D1TOTDIST20', 'D1TOTDIST15', 'D1TOTDIST10', 'D1totaldist0to15', 'D1totaldist0to30', 'D1TOTDIST5'],\n",
    "        ['D3TOTDIST5', 'D3TOTDIST30', 'D3TOTDIST25', 'D3TOTDIST20', 'D3TOTDIST15', 'D3totaldist0to30', 'D3totaldist0to15', 'D3TOTDIST10', 'D3hact0to15', 'D3hact0to30'],\n",
    "    ]\n",
    "    \n",
    "    for idx, batch in enumerate(batches):\n",
    "        print(f\"Writing batch_{idx} to disk ({len(batch)} fields)\")\n",
    "        df.filter(batch).to_csv(base_path / 'mice_data_set' / 'data' / f'phenomes_batch_{idx}.csv', header=True, index=False)\n",
    "\n",
    "create_manual_batches(phenomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
