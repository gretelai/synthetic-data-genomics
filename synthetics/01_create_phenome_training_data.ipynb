{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "base_path = pathlib.Path(os.getcwd().replace(\"/synthetics\", \"\"))\n",
    "experiment_path = base_path / 'mice_data_set' / 'out'\n",
    "    \n",
    "phenomes = pd.concat(map(pd.read_csv, experiment_path.glob('pheno_*.csv')))\n",
    "phenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMY COMMENTS:\n",
    "# Let's drop NA at the batch level only if the entire row is NaN. I added that to the pheno functional grouping\n",
    "# We don't need to filter out invalid measurements, because in re-reading the R prepare function that will be\n",
    "# taken care of before we save off the pheno data.\n",
    "# Sorry to be so vague about the relevant fields.  I think we can safely just use the fields mentioned in\n",
    "# pheno_functional_batching.ipynb now and that will take care of it\n",
    "# On the pheno, they seem to do the rounding they think makes sense in the pheno prepare call.  But in the geno\n",
    "# dataset, we definitely need to round.  I was rounding to whole ints, I think that makes sense.\n",
    "# So in a nutshell, we don't really need this clean_dataframe for the pheno data any longer. We just need the\n",
    "# code to do the batching in pheno_functional_batching.ipynb which also removes rows in the batch that\n",
    "# are all NaN\n",
    "\n",
    "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # Drop NaN fields\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Filter out invalid measurements\n",
    "    df = df[((df[\"discard\"] == \"no\") & (df[\"mixup\"] == \"no\"))]\n",
    "    \n",
    "    # Remove non-relevant fields\n",
    "    irrelevant_fields = pd.read_csv('irrelevant-fields.csv')\n",
    "    relevant_fields = list(set(phenomes.columns) - set(irrelevant_fields['FIELDS']))\n",
    "    df = df.filter(relevant_fields)\n",
    "    \n",
    "    # Remove unnecessary precision\n",
    "    df = df.round(2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "phenomes = clean_dataframe(phenomes)\n",
    "phenomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write synthetic training set to CSV\n",
    "phenomes.to_csv(base_path / 'mice_data_set' / 'data' / 'phenomes_train.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually batch training from matching covariates and write to CSV\n",
    "def create_manual_batches(df: pd.DataFrame):\n",
    "\n",
    "    batches = dict(\n",
    "        batch_0=['BMD', 'SW16', 'abBMD', 'TA', 'tibia', 'EDL', 'soleus', 'plantaris', 'gastroc', 'SW6', 'sacweight',\n",
    "                 'SW17', 'testisweight', 'methage', 'PPIweight', 'bw1'],\n",
    "        batch_1=['bw2', 'bw3', 'bw0', 'taillength', 'SW3', 'D3ctrtime0to30', 'D3ctrtime0to15', 'SW11', 'fastglucose',\n",
    "                 'SW1', 'glucoseage', 'SW19', 'SW7', 'SW22', 'SW14', 'SW24', 'SW20', 'SW10', 'SW9', 'SW4'],\n",
    "        batch_2=['AvToneD3', 'AvContextD2', 'AvToneD1', 'PreTrainD1', 'PPIbox1', 'FCbox1', 'methcage9', 'PPIbox2',\n",
    "                 'PPIbox3', 'FCbox2', 'methcage10', 'methcage11', 'FCbox3', 'PPIbox4', 'methcage7', 'methcage8',\n",
    "                 'methcage12'],\n",
    "        batch_3=['D3vact0to30', 'D3vact0to15', 'D1vact0to15', 'D1vact0to30', 'D2vact0to15', 'D2vact0to30',\n",
    "                 'D2ctrtime0to15', 'D2ctrtime0to30', 'D1ctrtime0to30', 'D1ctrtime0to15'],\n",
    "        batch_4=['D1hact0to15', 'D1hact0to30', 'D2hact0to30', 'D2hact0to15', 'D2TOTDIST5', 'D2TOTDIST15', 'D2TOTDIST10',\n",
    "                 'D2totaldist0to15', 'D2totaldist0to30', 'D2TOTDIST20', 'D2TOTDIST30', 'D2TOTDIST25', 'D1TOTDIST25',\n",
    "                 'D1TOTDIST30', 'D1TOTDIST20', 'D1TOTDIST15', 'D1TOTDIST10', 'D1totaldist0to15', 'D1totaldist0to30',\n",
    "                 'D1TOTDIST5'],\n",
    "        batch_5=['D3TOTDIST5', 'D3TOTDIST30', 'D3TOTDIST25', 'D3TOTDIST20', 'D3TOTDIST15', 'D3totaldist0to30',\n",
    "                 'D3totaldist0to15', 'D3TOTDIST10', 'D3hact0to15', 'D3hact0to30'])\n",
    "    \n",
    "    for key, batch in batches.items():\n",
    "        print(f\"Writing {key} to disk ({len(batch)} fields)\")\n",
    "        df.filter(batch).to_csv(base_path / 'mice_data_set' / 'data' / f'phenomes_{key}.csv', header=True, index=False)\n",
    "\n",
    "create_manual_batches(phenomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
